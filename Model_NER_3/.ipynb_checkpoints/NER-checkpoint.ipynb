{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "welcome-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.fr import French\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import minibatch\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "amended-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = French()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-republican",
   "metadata": {},
   "source": [
    "### Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "nominated-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path_json_file):\n",
    "    with open(path_json_file, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    return json_list\n",
    "\n",
    "path_json_file = \"/Users/remyadda/Desktop/AD/Projets/ALFRED/Datas/Doccano/doccano_skills_outputs_360.jsonl\"\n",
    "DATA = read_jsonl(path_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "valid-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformate_datas(TRAIN):\n",
    "    \"\"\"Input: JSON file issu de Doccano\n",
    "       Output: Datas mis au format du NER\"\"\"\n",
    "    TRAIN = []\n",
    "    for element in TRAIN_DATA:\n",
    "        # null pose pb. On le remplace par un mot qqconque\n",
    "        element = element.replace('null', '\"ok\"')\n",
    "        d = eval(element)\n",
    "        dictOfLabels = {}\n",
    "        dictOfLabels['entities'] = d['labels']\n",
    "        listOfTuples = []\n",
    "        for element in dictOfLabels['entities']:\n",
    "            element = tuple(element)\n",
    "            listOfTuples.append(element)\n",
    "        dictOfLabels['entities'] =  listOfTuples\n",
    "        TRAIN.append((d['text'], dictOfLabels))\n",
    "    return TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "median-allocation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n"
     ]
    }
   ],
   "source": [
    "DATA = reformate_datas(DATA)\n",
    "print(len(DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "weird-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train set :  288\n",
      "Len test set :  73\n"
     ]
    }
   ],
   "source": [
    "TRAIN = DATA[:int(0.8*len(DATA))]\n",
    "TEST = DATA[int(0.8*len(DATA)):]\n",
    "print('Len train set : ', len(TRAIN))\n",
    "print('Len test set : ', len(TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-exhaust",
   "metadata": {},
   "source": [
    "### Format Datas apres transformation --> (texte, listes de tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "illegal-dayton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LOGICIELS MAITRISES: WORD -- EXCEL -- SAGE -- POWERPOINT -- OUTLOOK',\n",
       " {'entities': [(21, 25, 'U-SKILLS'),\n",
       "   (29, 34, 'U-SKILLS'),\n",
       "   (38, 42, 'U-SKILLS'),\n",
       "   (46, 56, 'U-SKILLS'),\n",
       "   (60, 67, 'U-SKILLS')]})"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-monroe",
   "metadata": {},
   "source": [
    "### Modeling -- Ci dessous 2 differents NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "desirable-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(TRAIN):\n",
    "    # Load pre-existing spacy model\n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "    # Getting the pipeline component\n",
    "    ner=nlp.get_pipe(\"ner\")\n",
    "    # On va a present update ce pre trained NER avec NOS datas\n",
    "    # On ajoute nos LABELS au NER\n",
    "    for _, annotations in TRAIN:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    # Disable pipeline components you dont need to change\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # TRAINING THE MODEL\n",
    "    with nlp.disable_pipes(*unaffected_pipes):\n",
    "      # Training for 30 iterations\n",
    "      for iteration in range(30):\n",
    "        # shuufling examples  before every iteration\n",
    "        random.shuffle(TRAIN)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            for text, annotations in batch:\n",
    "                # create Example\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # Update the model\n",
    "                nlp.update([example], losses=losses, drop=0.3)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "alive-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2(LABEL, TRAIN):\n",
    "    # Setting up the pipeline and entity recognizer.\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('fr')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe('ner')\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    # Add new entity labels to entity recognizer\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)\n",
    "    # Inititalizing optimizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # Get names of other pipes to disable them during training to train # only NER and update the weights\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN, \n",
    "                                size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                for text, annotations in batch:\n",
    "                    # create Example\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    # Update the model\n",
    "                    nlp.update([example], losses=losses, drop=0.3)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "desirable-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "LABEL = [\"U-SKILLS\", \"B-SKILLS\", \"I-SKILLS\", \"L-SKILLS\", \"U-ANGLAIS\", \"U-NIVEAU_ANGLAIS\",\n",
    "    \"B-NIVEAU_ANGLAIS\", \"I-NIVEAU_ANGLAIS\", \"L-NIVEAU_ANGLAIS\", \"U-OTHER_LANGUAGE\"]\n",
    "    \n",
    "nlp = train_model_2(LABEL, TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-delta",
   "metadata": {},
   "source": [
    "### Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "floppy-character",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /Users/remyadda/Desktop/AD/Projets/ALFRED/ALFRED_PROJECT/Model_NER_3\n"
     ]
    }
   ],
   "source": [
    "def save_model(nlp, output_dir, model_name):\n",
    "    \"\"\"Input: nlp (model), directory ou l'on souhaite save le mpdel,\n",
    "    model_name est le nom qu'on souhaite donner a notre modele\"\"\"\n",
    "    # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "save_model(nlp, \"/Users/remyadda/Desktop/AD/Projets/ALFRED/ALFRED_PROJECT/Model_NER_3\", 'ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-costume",
   "metadata": {},
   "source": [
    "### Test on new datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "sorted-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ner_model(output_dir, test_text):\n",
    "    \"\"\"Permet de tester un NER\n",
    "    Input: directory du modele et texte a tester ainsi que le texte\n",
    "    Output: Renvoie les labels predits ainsi que le texte correspondant a chaque label\"\"\"\n",
    "    # Test the saved model\n",
    "    nlp = spacy.load(output_dir)\n",
    "    doc = nlp(test_text)\n",
    "    predicted_labels = []\n",
    "    corresponding_text = []\n",
    "    for ent in doc.ents:\n",
    "        predicted_labels.append(ent.label_)\n",
    "        corresponding_text.append(ent.text)\n",
    "    return predicted_labels, corresponding_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "metric-forum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LANGUES    Anglais : Courant  Espagnol : Debutant',\n",
       " {'entities': [(11, 18, 'U-ANGLAIS'),\n",
       "   (21, 28, 'U-NIVEAU_ANGLAIS'),\n",
       "   (30, 38, 'U-OTHER_LANGUAGE')]})"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "color-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_trueLabelAndWord(DATASET):\n",
    "    \"\"\"Permet de recuperer deux listes: True label, et le word correspondant\n",
    "    Ceci permettra de comparer les valeurs reelles aux valeurs predites du NER\"\"\"\n",
    "    true_labels = []\n",
    "    corresponding_text = []\n",
    "    for element in DATASET:\n",
    "        # on retrieve le texte\n",
    "        text = element[0]\n",
    "        label = element[1]\n",
    "        label = label['entities']\n",
    "        for e in label:\n",
    "            start = e[0]\n",
    "            end = e[1]\n",
    "            target = e[2]\n",
    "            true_labels.append(target)\n",
    "            corresponding_word = text[start:end]\n",
    "            corresponding_text.append(corresponding_word)\n",
    "    return true_labels, corresponding_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "boxed-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_test(DATASET):\n",
    "    \"\"\"Permet de recuperer le texte afin d'effectuer les predictions\"\"\"\n",
    "    texts = []\n",
    "    for element in DATASET:\n",
    "        text = element[0]\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "descending-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUES    Anglais : Courant  Espagnol : Debutant\n"
     ]
    }
   ],
   "source": [
    "Xtest = X_test(TEST)\n",
    "print(Xtest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-census",
   "metadata": {},
   "source": [
    "#### 1- Récupération des tokens et labels que le modèle devrait être capable de prédire (True tuples on les appelera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "addressed-darkness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels :  U-ANGLAIS  --> True corresponding word :  Anglais\n",
      "410 tuples\n"
     ]
    }
   ],
   "source": [
    "true_labels, true_corresponding_words = retrieve_trueLabelAndWord(TEST)\n",
    "print('True labels : ', true_labels[0], ' --> True corresponding word : ',true_corresponding_words[0])\n",
    "# Create list of tuples (word, label_predicted) --> useful for metrics\n",
    "\n",
    "def make_tuples(true_token, true_labels):\n",
    "    \"\"\"Input: la liste de mots (token) ainsi que la liste de labels correspondante\n",
    "       Output: liste de tuples ou chaque tuple est de la forme (token, label)\"\"\"\n",
    "    true_tuple = []\n",
    "    for w, p in zip(true_token, true_labels):\n",
    "        true_tuple.append((w, p))\n",
    "\n",
    "    print(len(true_tuple), 'tuples')\n",
    "    return true_tuple\n",
    "\n",
    "# Nbe de tuples a trouver\n",
    "true_tuple = make_tuples(true_corresponding_words, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-legislation",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "governmental-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On predit chaque texte de Xtest\n",
    "preds = []\n",
    "predicted_corresponding_word_list = []\n",
    "for text in Xtest:\n",
    "    predictions, predicted_corresponding_word = test_ner_model(output_dir, text)\n",
    "    preds.extend(predictions)\n",
    "    predicted_corresponding_word_list.extend(predicted_corresponding_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-discovery",
   "metadata": {},
   "source": [
    "#### 2 - Récupération des tokens et labels que le modèle a prédit (predicted tuples on les appelera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "recreational-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 tuples\n"
     ]
    }
   ],
   "source": [
    "# Create list of tuples (word, label_predicted) --> useful for metrics\n",
    "# Nbe de tuples prédits\n",
    "predicted_tuple = make_tuples(predicted_corresponding_word_list, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-animal",
   "metadata": {},
   "source": [
    "### Defintion des Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "generous-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explication : supposons que dans le Test set il y ait 200 mots a labeliser, et que le modele en trouve 100, \n",
    "# alors, le coverage serait de 50%. Certes toutes les predictions peuvent etre bonnes, mais il en manque la moitié.\n",
    "def get_metrics(y_pred, y_true):\n",
    "    \"\"\"Input: y_pred et y_true sous forme de listes de tuple (texte, label)\n",
    "    Renvoie le poucentage de bons tuples (texte, label) détectés par le modele\"\"\"\n",
    "    y_true_copy = y_true.copy()\n",
    "    y_pred_copy = y_pred.copy()\n",
    "    wrong_tuples = []\n",
    "    good_tuples = []\n",
    "    count = 0\n",
    "    for tuple_ in y_pred_copy:\n",
    "        count += 1\n",
    "        if tuple_ in y_true_copy:\n",
    "            y_true_copy.remove(tuple_)\n",
    "            good_tuples.append(tuple_)\n",
    "        else:\n",
    "            # tuple qui na pas lieu d'exister\n",
    "            wrong_tuples.append(tuple_)\n",
    "    #print(count)\n",
    "    accuracy_1 = np.round(len(good_tuples)/len(y_pred), 3) # cb de bonnes predictions par rap a ce que je predis\n",
    "    accuracy_2 = np.round(len(good_tuples)/len(y_true), 3) # % cb de bonnes predictions par rap a ce quil fallait trouver\n",
    "    \n",
    "    #lets take a look on bad predictions\n",
    "    # erreur de type 1 : mot trouvé, mais prédiction fausse\n",
    "    label_error = {} # on spotera les labels où le modèle fait erreur\n",
    "    count_error_I = 0\n",
    "    for tuple_ in wrong_tuples:\n",
    "        for TrueTuple in y_true_copy:\n",
    "            if tuple_[0] == TrueTuple[0]:\n",
    "                count_error_I += 1\n",
    "                if (tuple_[1], TrueTuple[1]) not in label_error:\n",
    "                    label_error[(tuple_[1], TrueTuple[1])] = 1\n",
    "                else:\n",
    "                    label_error[(tuple_[1], TrueTuple[1])] += 1\n",
    "                y_true_copy.remove(TrueTuple)\n",
    "                break\n",
    "        # Si c'est pas une erreur de type I c'est qu'il s'agit d'une erreur de type II \n",
    "        # cad un mot releve qui n'aurait jamais du l'etre\n",
    "    count_error_II = len(wrong_tuples) - count_error_I\n",
    "    coverage = np.round(len(y_true) / len(y_pred) * 100, 1)\n",
    "    coverage = np.round(np.abs(100 - coverage), 1)\n",
    "    print(\"Nombre de mots à prédire pour l'ensemble du test set : \", len(y_true))\n",
    "    print(\"Nombre de mots prédits par le NER pour l'ensemble du test set : \", len(y_pred))\n",
    "    print('Coverage : ', coverage, '% de difference')\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print(len(good_tuples), ' mots ont bien été relevés ET prédits par le NER')\n",
    "    print(len(wrong_tuples), \" mots ont mal été prédits (erreur de type I ou type II)\")\n",
    "    print('Bonnes prédictions + Mauvaises prédictions =', len(good_tuples)+len(wrong_tuples))\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print('Accuracy I (bonne pred / total pred) : ', accuracy_1*100, ' %')\n",
    "    print('Accuracy II (bonne pred / total expected pred) : ', accuracy_2*100, ' %')\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print('Nbe erreur type I : ', count_error_I, ' (mots bien relevés mais mal prédits)')\n",
    "    print('Nbe erreur type II : ', count_error_II, \" (mots qui n'auraient pas du être prédits)\")\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print(\"Confusions faites par l'algo (prediction, true_label): \\n\", label_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-galaxy",
   "metadata": {},
   "source": [
    "#### Lets check our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "considerable-pontiac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots à prédire pour l'ensemble du test set :  410\n",
      "Nombre de mots prédits par le NER pour l'ensemble du test set :  403\n",
      "Coverage :  1.7 % de difference\n",
      "----------------------------------------------------------------------\n",
      "326  mots ont bien été relevés ET prédits par le NER\n",
      "77  mots ont mal été prédits (erreur de type I ou type II)\n",
      "Bonnes prédictions + Mauvaises prédictions = 403\n",
      "----------------------------------------------------------------------\n",
      "Accuracy I (bonne pred / total pred) :  80.9  %\n",
      "Accuracy II (bonne pred / total expected pred) :  79.5  %\n",
      "----------------------------------------------------------------------\n",
      "Nbe erreur type I :  11  (mots bien relevés mais mal prédits)\n",
      "Nbe erreur type II :  66  (mots qui n'auraient pas du être prédits)\n",
      "----------------------------------------------------------------------\n",
      "Confusions faites par l'algo (prediction, true_label): \n",
      " {('L-SKILLS', 'U-SKILLS'): 1, ('U-SKILLS', 'B-SKILLS'): 6, ('U-SKILLS', 'L-SKILLS'): 1, ('U-NIVEAU_ANGLAIS', 'B-NIVEAU_ANGLAIS'): 1, ('B-SKILLS', 'U-SKILLS'): 2}\n"
     ]
    }
   ],
   "source": [
    "get_metrics(predicted_tuple, true_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-former",
   "metadata": {},
   "source": [
    "#### Pour appliquer cette fonction metric, il faut d'abord diviser votre DATASET en TRAIN et TEST.\n",
    "    Puis, dans le cas ou votre NER prend ceci en INPUT : \n",
    "\n",
    "    INPUT :\n",
    "    ('LOGICIELS MAITRISES: WORD -- EXCEL -- SAGE -- POWERPOINT -- OUTLOOK',\n",
    "     {'entities': [(21, 25, 'U-SKILLS'),\n",
    "       (29, 34, 'U-SKILLS'),\n",
    "       (38, 42, 'U-SKILLS'),\n",
    "       (46, 56, 'U-SKILLS'),\n",
    "       (60, 67, 'U-SKILLS')]}), \n",
    "   \n",
    "    entrainez votre NER sur le train set.\n",
    "    \n",
    "    Afin de tester ses performances sur le test, recuperer les mots ainsi que les preditions qu'il aurait du       pouvoir effectuer dans le meilleur des cas, pour cela, utilisez la fonction \"retrieve_trueLabelAndWord\" qui prend en input l'INPUT ci dessus, et renvoie deux listes : true label et mots à predire.\n",
    "    \n",
    "        Utiliser ensuite la fonction \"make_tuple\" afin de merge ces deux listes en une seule liste de tuples.\n",
    "        On utilise deux fois cette fonction : \n",
    "        - pour les reels labels a trouver\n",
    "        - pour les prediction effectuées par l'algo\n",
    "        \n",
    "        Enfin on applique la fonction get_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-cooper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
