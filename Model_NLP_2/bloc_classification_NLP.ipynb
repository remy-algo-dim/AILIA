{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "overall-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection as sms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from spacy.lang.fr import French\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "straight-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/remyadda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/remyadda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-victorian",
   "metadata": {},
   "source": [
    "## Construction d'un modèle n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fossil-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne tiendra pas compte de l'ordre des mots\n",
    "# On load les données\n",
    "path_data = \"db-text-for-bloc-labelization-nlp.csv\"\n",
    "df = pd.read_csv(path_data)\n",
    "# le CSV est une simple df a deux colonnes: C1=texte -- C2=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nearby-portal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info_perso</td>\n",
       "      <td>Maeva DENISE SAUTDELIGNE 2 Rue Robert Schuman ...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skills</td>\n",
       "      <td>LOGICIELS MAITRISES: WORD -- EXCEL -- SAGE -- ...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skills</td>\n",
       "      <td>COMPETENCES SAUTDELIGNE  SAUTDELIGNE VV VV VV ...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>experience_pro</td>\n",
       "      <td>EXPERIENCES PROFESSIONNELLE SAUTDELIGNE  SAUTD...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formation</td>\n",
       "      <td>PROT AR OL AVEO) BS SAUTDELIGNE  SAUTDELIGNE &gt;...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label                                               text  \\\n",
       "0      info_perso  Maeva DENISE SAUTDELIGNE 2 Rue Robert Schuman ...   \n",
       "1          skills  LOGICIELS MAITRISES: WORD -- EXCEL -- SAGE -- ...   \n",
       "2          skills  COMPETENCES SAUTDELIGNE  SAUTDELIGNE VV VV VV ...   \n",
       "3  experience_pro  EXPERIENCES PROFESSIONNELLE SAUTDELIGNE  SAUTD...   \n",
       "4       formation  PROT AR OL AVEO) BS SAUTDELIGNE  SAUTDELIGNE >...   \n",
       "\n",
       "                                             file  \n",
       "0  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg  \n",
       "1  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg  \n",
       "2  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg  \n",
       "3  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg  \n",
       "4  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "former-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = French()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-presentation",
   "metadata": {},
   "source": [
    "## Some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "synthetic-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence, stopwords):\n",
    "    # On crée notre objet token, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "    # On lemmatize chaque token et on convertit en lowercase (lemmatization=forme cannonique/standard d'un mot)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    mytokens = [lemmatizer.lemmatize(str(word)).lower() for word in mytokens if not     \n",
    "          word.is_punct and not word.like_num and word.text != 'n']\n",
    "    # Removing stop words\n",
    "    #print(mytokens)\n",
    "    mytokens = [word for word in mytokens if word not in stopwords.words()]\n",
    "    mytokens = [word for word in mytokens if word != 'sautdeligne']\n",
    "    # Remove accents\n",
    "    #mytokens = [strip_accents_ascii(word) for word in mytokens]\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Limitation du nombre de features\n",
    "TOP_K = 20000\n",
    "# On split le texte en \"mots\" ou \"n-grams\"\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "existing-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-binary",
   "metadata": {},
   "source": [
    "## La cellule suivante met du temps a run (5mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "informational-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.tolist()\n",
    "new_text = [spacy_tokenizer(s, stopwords) for s in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fleet-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_text'] = [' '.join(s) for s in new_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pharmaceutical-stage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info_perso</td>\n",
       "      <td>Maeva DENISE SAUTDELIGNE 2 Rue Robert Schuman ...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "      <td>maeva denise rue robert schuman   villeneuve-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skills</td>\n",
       "      <td>LOGICIELS MAITRISES: WORD -- EXCEL -- SAGE -- ...</td>\n",
       "      <td>CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg</td>\n",
       "      <td>logiciels maitrises word excel sage powerpoint...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text  \\\n",
       "0  info_perso  Maeva DENISE SAUTDELIGNE 2 Rue Robert Schuman ...   \n",
       "1      skills  LOGICIELS MAITRISES: WORD -- EXCEL -- SAGE -- ...   \n",
       "\n",
       "                                             file  \\\n",
       "0  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg   \n",
       "1  CV ASSISTANTE DE DIRECTION _ DENISE Maëva.jpg   \n",
       "\n",
       "                                            new_text  \n",
       "0  maeva denise rue robert schuman   villeneuve-l...  \n",
       "1  logiciels maitrises word excel sage powerpoint...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "infinite-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116  rows in our DB\n"
     ]
    }
   ],
   "source": [
    "print(len(df), ' rows in our DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "successful-evening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'competences   langues anglais langue maternelle   creole jamaique sierra leone   teed     poular guinee   >   sos   boc   susu guinee     bot   mende sierra leone   informatique logiciels     bot   pack office dax   lv talent ardian tetrawin simm   akio     bot     bot     bot   ss     ss     ss  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_text'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-productivity",
   "metadata": {},
   "source": [
    "#### Il reste pas mal de data cleaning a faire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-houston",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bulgarian-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "data_train = df[:820]\n",
    "data_valid = df[820:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "colored-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "    # Inputs\n",
    "      train_texts: list, training text strings.\n",
    "      train_labels: np.ndarray, training labels.\n",
    "      val_texts: list, validation text strings.\n",
    "    # Outputs \n",
    "       x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Paramètres qu'on passe au TF-IDF\n",
    "    kwargs = {\n",
    "      'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "      'dtype': 'int32',\n",
    "      'strip_accents': 'unicode',\n",
    "      'decode_error': 'replace', \n",
    "      'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "       'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "       }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "julian-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remyadda/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1808: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_val = ngram_vectorize(data_train[\"new_text\"], data_train[\"label\"], data_valid[\"new_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-tucson",
   "metadata": {},
   "source": [
    "## Choix du modèle\n",
    "### a - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "substantial-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[\"label\"]\n",
    "y_test = data_valid[\"label\"]\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(x_train, y_train)\n",
    "predicted = NB_clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tamil-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "experience_pro       0.81      1.00      0.89        68\n",
      "     formation       1.00      0.91      0.95        65\n",
      "    info_perso       1.00      0.95      0.98        65\n",
      "        skills       0.95      0.88      0.91        98\n",
      "\n",
      "      accuracy                           0.93       296\n",
      "     macro avg       0.94      0.93      0.93       296\n",
      "  weighted avg       0.94      0.93      0.93       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-singapore",
   "metadata": {},
   "source": [
    "### b - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "southeast-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "experience_pro       0.94      0.90      0.92        68\n",
      "     formation       0.94      0.95      0.95        65\n",
      "    info_perso       0.90      1.00      0.95        65\n",
      "        skills       0.98      0.93      0.95        98\n",
      "\n",
      "      accuracy                           0.94       296\n",
      "     macro avg       0.94      0.94      0.94       296\n",
      "  weighted avg       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_clf =BaggingClassifier(DecisionTreeClassifier(),n_estimators=50, n_jobs=-1)\n",
    "bag_clf.fit(x_train, y_train)\n",
    "predicted = bag_clf.predict(x_val)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-salvation",
   "metadata": {},
   "source": [
    "## Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vocal-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'model_block_classification.pkl'\n",
    "pickle.dump(bag_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "universal-continent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skills' 'experience_pro' 'skills' 'info_perso' 'experience_pro'\n",
      " 'formation' 'skills' 'info_perso' 'experience_pro' 'formation' 'skills'\n",
      " 'info_perso' 'skills' 'experience_pro' 'formation' 'info_perso'\n",
      " 'experience_pro' 'skills' 'formation' 'skills' 'info_perso' 'formation'\n",
      " 'experience_pro' 'skills' 'info_perso' 'skills' 'formation'\n",
      " 'experience_pro' 'formation' 'skills' 'experience_pro' 'info_perso'\n",
      " 'formation' 'skills' 'experience_pro' 'info_perso' 'experience_pro'\n",
      " 'formation' 'skills' 'info_perso' 'info_perso' 'skills' 'experience_pro'\n",
      " 'formation' 'skills' 'formation' 'experience_pro' 'info_perso' 'skills'\n",
      " 'skills' 'experience_pro' 'formation' 'info_perso' 'skills' 'info_perso'\n",
      " 'formation' 'experience_pro' 'skills' 'info_perso' 'experience_pro'\n",
      " 'formation' 'skills' 'info_perso' 'formation' 'experience_pro'\n",
      " 'info_perso' 'skills' 'formation' 'skills' 'experience_pro' 'info_perso'\n",
      " 'skills' 'info_perso' 'formation' 'experience_pro' 'skills' 'info_perso'\n",
      " 'experience_pro' 'formation' 'skills' 'skills' 'info_perso' 'skills'\n",
      " 'formation' 'experience_pro' 'skills' 'formation' 'skills' 'info_perso'\n",
      " 'experience_pro' 'formation' 'skills' 'skills' 'info_perso' 'formation'\n",
      " 'experience_pro' 'info_perso' 'skills' 'experience_pro' 'formation'\n",
      " 'skills' 'info_perso' 'skills' 'info_perso' 'experience_pro' 'formation'\n",
      " 'skills' 'info_perso' 'skills' 'formation' 'experience_pro'\n",
      " 'experience_pro' 'formation' 'skills' 'info_perso' 'info_perso'\n",
      " 'formation' 'formation' 'skills' 'skills' 'info_perso' 'experience_pro'\n",
      " 'formation' 'skills' 'skills' 'info_perso' 'experience_pro' 'formation'\n",
      " 'skills' 'experience_pro' 'formation' 'skills' 'info_perso' 'info_perso'\n",
      " 'formation' 'skills' 'info_perso' 'skills' 'info_perso' 'formation'\n",
      " 'experience_pro' 'skills' 'info_perso' 'formation' 'formation'\n",
      " 'info_perso' 'info_perso' 'info_perso' 'formation' 'skills'\n",
      " 'experience_pro' 'experience_pro' 'formation' 'skills' 'info_perso'\n",
      " 'formation' 'experience_pro' 'skills' 'info_perso' 'info_perso' 'skills'\n",
      " 'skills' 'skills' 'formation' 'experience_pro' 'info_perso' 'skills'\n",
      " 'skills' 'skills' 'formation' 'experience_pro' 'info_perso' 'skills'\n",
      " 'skills' 'experience_pro' 'formation' 'info_perso' 'info_perso'\n",
      " 'experience_pro' 'formation' 'skills' 'info_perso' 'skills' 'formation'\n",
      " 'experience_pro' 'experience_pro' 'experience_pro' 'formation' 'skills'\n",
      " 'skills' 'experience_pro' 'info_perso' 'info_perso' 'experience_pro'\n",
      " 'formation' 'skills' 'info_perso' 'formation' 'experience_pro' 'skills'\n",
      " 'info_perso' 'experience_pro' 'skills' 'formation' 'experience_pro'\n",
      " 'info_perso' 'formation' 'skills' 'experience_pro' 'info_perso'\n",
      " 'formation' 'formation' 'skills' 'info_perso' 'skills' 'formation'\n",
      " 'skills' 'experience_pro' 'info_perso' 'experience_pro' 'skills'\n",
      " 'formation' 'skills' 'info_perso' 'formation' 'experience_pro' 'skills'\n",
      " 'formation' 'experience_pro' 'info_perso' 'experience_pro' 'info_perso'\n",
      " 'skills' 'skills' 'skills' 'formation' 'experience_pro' 'info_perso'\n",
      " 'info_perso' 'experience_pro' 'skills' 'formation' 'info_perso' 'skills'\n",
      " 'formation' 'experience_pro' 'experience_pro' 'experience_pro'\n",
      " 'info_perso' 'skills' 'info_perso' 'experience_pro' 'formation' 'skills'\n",
      " 'info_perso' 'skills' 'formation' 'experience_pro' 'info_perso' 'skills'\n",
      " 'skills' 'info_perso' 'info_perso' 'skills' 'experience_pro' 'formation'\n",
      " 'skills' 'info_perso' 'skills' 'experience_pro' 'formation' 'skills'\n",
      " 'info_perso' 'skills' 'experience_pro' 'formation' 'skills' 'info_perso'\n",
      " 'formation' 'experience_pro' 'skills' 'skills' 'info_perso' 'skills'\n",
      " 'experience_pro' 'formation' 'skills' 'skills' 'experience_pro'\n",
      " 'formation' 'info_perso' 'info_perso' 'experience_pro' 'formation'\n",
      " 'skills' 'skills']\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict(x_val)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
